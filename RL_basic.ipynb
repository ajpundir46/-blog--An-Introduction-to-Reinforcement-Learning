{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4Ap6qqqtRZQ"
      },
      "source": [
        "# 1. Model based "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkcxdE3-tf7Y"
      },
      "source": [
        "## Create environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8TDMJQVttGo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.grid = np.zeros((size, size))\n",
        "        self.start = (0, 0)\n",
        "        self.goal = (size-1, size-1)  #last block\n",
        "\n",
        "    def step(self, action):\n",
        "        i, k = self.start\n",
        "        j=k\n",
        "        # take action\n",
        "        if action == 0:  # up\n",
        "            i = max(i-1, 0)\n",
        "        elif action == 1:  # down\n",
        "            i = min(i+1, self.size-1)\n",
        "        elif action == 2:  # left\n",
        "            j = max(j-1, 0)\n",
        "        elif action == 3:  # right\n",
        "            j = min(j+1, self.size-1)\n",
        "\n",
        "        # calculate reward and update state\n",
        "        if (i, j) == self.goal:\n",
        "            reward = 1\n",
        "            done = True\n",
        "        else:\n",
        "            reward = 0\n",
        "            done = False\n",
        "\n",
        "        self.start = (i, j)\n",
        "\n",
        "        return (i, j), reward, done\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zDPQm_e5YI3"
      },
      "source": [
        "In this environment, the agent starts in the top-left corner and must navigate to the bottom-right corner to reach the goal state. The step function takes an action (0 = up, 1 = down, 2 = left, 3 = right) and returns the next state, reward, and a boolean indicating whether the episode has terminated.\n",
        "\n",
        "Note that this environment is considered model-based because we have explicitly defined the transition probabilities (i.e., the new state is determined by the action taken and the current state) and rewards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CY1i-_OvtY5"
      },
      "source": [
        "##1.1   Value based Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbTqZrEKtPks"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, env, alpha=0.1, gamma=0.9, eps=0.1):\n",
        "        self.env = env\n",
        "        self.alpha = alpha  # learning rate\n",
        "        self.gamma = gamma  # discount factor\n",
        "        self.eps = eps  # exploration rate\n",
        "        self.Q = np.zeros((env.size, env.size, 4))  # 4 represents total number of actions\n",
        "    def act(self, state):\n",
        "        if 0 < self.eps:\n",
        "            return np.random.randint(4)  # choose random action\n",
        "        else:\n",
        "            return np.argmax(self.Q[state])\n",
        "\n",
        "    def train(self, episodes):\n",
        "        for episode in range(episodes):\n",
        "            state = self.env.start\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                action = self.act(state)\n",
        "                next_state, reward, done = self.env.step(action)\n",
        "\n",
        "                # update Q-value for current state-action pair\n",
        "               \n",
        "                max_next_q = np.max(self.Q[next_state]) if not done else 0 \n",
        "  \n",
        "                td_target = reward + self.gamma * max_next_q\n",
        "                td_error = td_target - self.Q[state][action]\n",
        "                self.Q[state][action] += self.alpha * td_error\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "    def test(self):\n",
        "        state = self.env.start\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = np.argmax(self.Q[state])\n",
        "            print(\"action{}\".format(action))\n",
        "            next_state, reward, done = self.env.step(action)\n",
        "            state = next_state\n",
        "\n",
        "        return reward,self.Q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciI62PpJvc36"
      },
      "outputs": [],
      "source": [
        "env = GridWorld(4)\n",
        "agent = QLearningAgent(env)\n",
        "agent.train(1000)\n",
        "reward,Q= agent.test()\n",
        "\n",
        "\n",
        "print(f\"Reached goal with reward {reward}\")\n",
        "print(\"Q value list is as follow:\")\n",
        "print(Q)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-zpVpMNwXQ4"
      },
      "source": [
        "This code will train the agent for 1000 episodes and then test it by navigating from the start state to the goal state using the learned Q-table. The output will show whether the agent successfully reached the goal state (reward=1) or not (reward=0) with updated Q table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPoVAxwA5yru"
      },
      "source": [
        "# 2. Model Free "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgjY6_gI3_eu"
      },
      "source": [
        "## Create environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex9C-NkRwU6R"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.state_space = ['s0', 's1', 's2', 's3']\n",
        "        self.action_space = ['a0', 'a1', 'a2']\n",
        "        self.reward_space = {'s0': {'a0': 1, 'a1': 4, 'a2': 0},\n",
        "                             's1': {'a0': 4, 'a1': 1, 'a2': 4},\n",
        "                             's2': {'a0': 5, 'a1': 0, 'a2': 2},\n",
        "                             's3': {'a0': 0, 'a1': 3, 'a2': 0}}\n",
        "        self.current_state = random.choice(self.state_space)\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = self.reward_space[self.state_space[self.current_state]][self.action_space[action]]\n",
        "        next_state = random.choice(range(len(state_space)))\n",
        "        self.current_state = next_state\n",
        "        return next_state, reward\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_state = random.choice(range(len(state_space)))\n",
        "        return self.current_state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pmgpm8U-4p1J"
      },
      "source": [
        "In this example, we have a state space of four states ('s0', 's1', 's2', 's3'), an action space of three actions ('a0', 'a1', 'a2'), and a reward space that maps each state-action pair to a reward. The transition dynamics are random, with the next state chosen uniformly at random from the state space. The step function takes an action as input and returns the next state, reward, and a done flag that indicates whether the episode is finished. The reset function resets the environment to a random initial state and returns that state. This is a simple example, but in practice, the state space and action space could be much larger and the transition dynamics more complex."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olpVAeuK5k9e"
      },
      "source": [
        "##1.1   Value based Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PegyJEa5K3-y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class QLearningAgent1:\n",
        "    def __init__(self, env, alpha=0.1, gamma=0.9, eps=0.1):\n",
        "        self.env = env\n",
        "        self.alpha = alpha  # learning rate\n",
        "        self.gamma = gamma  # discount factor\n",
        "        self.eps = eps  # exploration rate\n",
        "        self.Q = np.zeros((len(env.state_space), len(env.action_space)))  # 3 represents total number of actions\n",
        "    def act(self, state):\n",
        "        if 0 < self.eps:\n",
        "            return np.random.randint(3)  # choose random action\n",
        "        else:\n",
        "            return np.argmax(self.Q[state])\n",
        "\n",
        "    def train(self, episodes, trajsize):\n",
        "        for episode in range(episodes):\n",
        "            state = self.env.reset()\n",
        "            while trajsize>0:\n",
        "                action = self.act(state)\n",
        "                next_state, reward = self.env.step(action)\n",
        "\n",
        "                # update Q-value for current state-action pair\n",
        "               \n",
        "                max_next_q = np.max(self.Q[next_state])  \n",
        "  \n",
        "                td_target = reward + self.gamma * max_next_q\n",
        "                td_error = td_target - self.Q[state][action]\n",
        "                self.Q[state][action] += self.alpha * td_error\n",
        "\n",
        "                state = next_state\n",
        "                trajsize-=1\n",
        "\n",
        "    def test(self, trajsize):\n",
        "        state = self.env.reset()\n",
        "\n",
        "        while trajsize>0:\n",
        "            action = np.argmax(self.Q[state])\n",
        "            next_state, reward = self.env.step(action)\n",
        "            state = next_state\n",
        "            trajsize-=1\n",
        "        return reward,self.Q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DnvLPXq5kNX",
        "outputId": "9a398a60-f769-4ff1-9481-026431c3e84f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reached goal with reward 3\n",
            "Q value list is as follow:\n",
            "[[0.41966151 1.93320758 0.33521994]\n",
            " [0.82879939 0.44786282 1.55650642]\n",
            " [1.58165659 0.43575932 1.19501751]\n",
            " [0.31547024 0.71289493 0.15782569]]\n"
          ]
        }
      ],
      "source": [
        "env = Environment()\n",
        "agent = QLearningAgent1(env)\n",
        "agent.train(1000, 50)\n",
        "reward,Q= agent.test(4)\n",
        "\n",
        "print(f\"Reached goal with reward {reward}\")\n",
        "print(\"Q value list is as follow:\")\n",
        "print(Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elQWMbySpyra"
      },
      "source": [
        "This code will train the agent for 1000 episodes and trajectory 50 and then test it by navigating from the reset state to the goal state using the learned Q-table. The output will show whether the agent successfully reached the goal state (reward=1) or not (reward=0) with updated Q table."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
